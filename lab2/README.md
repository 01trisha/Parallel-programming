## Вариант задачи:
Метод простой итерации

## Запуск
### main:
    gcc main.c -o main -lm
### omp_parallel_for.c:
    на сервере: gcc -fopenmp omp_parallel_for.c -o omp_parallel_for -lm 
    на мак: clang -fopenmp=libomp omp_parallel_for.c -o omp_parallel_for
    export OMP_NUM_THREADS=N
    ./omp_parallel_for

### omp_parallel.c:
    на сервере: gcc -fopenmp -o omp_parallel omp_parallel.c -lm
    на мак: clang -fopenmp -o omp_parallel omp_parallel.c
    export OMP_NUM_THREADS=N
    ./omp_parallel

### omp_parallel_schedule.c:
    chmod +x run_schedule.sh
    на сервере: gcc -fopenmp -o omp_parallel_schedule omp_parallel_schedule.c -lm
    на мак: clang -fopenmp -o omp_parallel_schedule omp_parallel_schedule.c  
    ./run_schedule.sh


При условиях нашей матрицы A у нас определяется максимальное TAU по формуле 2/(N+1)
поэтому была для большого N и при этом нормального вектора X использую TAU = 0.0001

в последующих использую нормальное TAU = 0.01 для значимого времени выполнения

## время выполнения main
67.404712 sec
## время выполнения omp_parallel:
threads = 1: 67.114254 sec
threads = 2: 33.871753 sec
threads = 4: 19.229057 sec
threads = 8: 9.703501 sec
threads = 16: 8.821988 sec
## время выполнения omp_parallel_for:
threads = 1: 68.392633 sec
threads = 2: 33.878401 sec
threads = 4: 19.211936 sec
threads = 8: 9.737794 sec
threads = 16: 8.764055 sec
## вывод с schedule
DYNAMIC
threads = 4
default chunk
Time: 41.859996 sec
chunk = 1
Time: 41.632447 sec
chunk = 5
Time: 32.459322 sec
chunk = 10
Time: 19.201062 sec
chunk = 20
Time: 17.599500 sec
chunk = 50
Time: 18.648499 sec
chunk = 100
Time: 20.157584 sec
threads = 8
Time: 34.113778 sec
chunk = 1
Time: 34.012110 sec
chunk = 5
Time: 18.582897 sec
chunk = 10
Time: 9.839091 sec
chunk = 20
Time: 9.479810 sec
chunk = 50
Time: 9.669275 sec
chunk = 100
Time: 10.892455 sec
threads = 16
Time: 27.403376 sec
chunk = 1
Time: 27.480009 sec
chunk = 5
Time: 11.956683 sec
chunk = 10
Time: 6.880032 sec
chunk = 20
Time: 6.779626 sec
chunk = 50
Time: 7.586969 sec
chunk = 100
Time: 8.444123 sec

поток (выполнения) – объект управления операционной системы, который служит для организации выполнения последовательных вычислений (если не рассматривать параллелизм на уровне ядра процессора при реализации последовательных команд потока).

Когда операционная система принимает решение приостановить поток, чтобы возобновить его работу на этом или другом ядре, то состояние регистров ядра запоминается и восстанавливается на ядре нового размещения для того, чтобы поток продолжил работу с места, на котором был остановлен.

Преимущества использования OpenMP API по сравнению с про-
граммированием потоков в базовых средствах операционных сис-
тем (POSIX Threads – см. раздел 4.3, Win32 API Threads):
1. Более высокий уровень программирования: программист
указывает, какие части программы могут быть выполнены незави-
симо друг от друга, но не занимается непосредственно управлени-
ем потоками операционной системы;
2. Эффективная переносимость: программы, следующие специ-
фикациям OpenMP, исполняются на вычислительных системах раз-
личных производителей без изменений, для их сборки требуется
только компилятор, реализующий соответствующую специфика-
цию; при этом в эффективных реализациях OpenMP API сгенериро-
ванный код обращается непосредственно к функциям управления
потоками целевой операционной системы, в то время как, напри-
мер, программа, написанная с использованием спецификации POSIX
Threads API, требует в системах типа Windows библиотеки, реали-
зующей программный интерфейс POSIX Threads через обращения
к Win32 API Threads.
3. Относительно легкая трансформация имеющегося последо-
вательного кода в параллельный.

способ узнать кол-во потоков - ulimit -u

#pragma omp parallel for указывает компилятору, что итерации цикла не зависят друг от друга и могут быть выполнены параллельно. 

Переменные внутри параллельной области делятся на два клас-
са: частные (private) и разделяемые (shared).
В примерах на листингах 4.2 и 4.3 переменная id относится
к классу частных переменных: в каждом потоке это имя идентифи-
цирует собственную область памяти. Поэтому, когда потоки при-
сваивают этой переменной значение функции omp_get_thread_
thread_num(), каждый поток получает собственное значение
этой переменной. Все переменные, объявленные в пределах парал-
лельной области, относятся к классу частных переменных. Индекс-
ная переменная распараллеливаемого цикла for является частной,
даже если ее объявление было сделано вне конструкции цикла.
Таким образом, каждый поток команды, выполняющей параллель-
ный цикл, имеет собственную область памяти для индексной пере-
менной и меняет ее значение независимо от других потоков


у dynamic есть планировщик который выдает задачи потоку и может произойти ситуация когда в порядке очереди будут выдаваться задачи , пока выдается, второй может простаивать и из за этого простой по времени
допустим 2 потока по 4 итерации, 1 поток отработал, смотрит какие итерации свободны и берет их 
а у статика во время работы уже вся работа распределена на каждый поток

reduction создает копию на каждый поток и независимо потоки кладут сумму свою а потом после цикла они все суммируются и кладут в указанную переменную

shared - потоки могут совместно к ней обращаться
private - аждый поток получает свою локальную копию переменной

1. static - делит итерации равномерно между потоками 
2. static, chunk_size - каждому потоку назначается N итераций за раз 
3. dynamic - потоки берут по одной итерации динамически 
4. dynamic, chunk_size - потоки берут N итераций за раз 
5. guided, chunk_size - потоки получают уменьшающиеся блоки 
6. auto - OpenMP сам выбирает стратегию