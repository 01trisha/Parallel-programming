# MPI

# Описание:
1. Пусть есть система из N линейных алгебраических уравнений в виде
Ax=b, где А – матрица коэффициентов уравнений размером N×N, b–вектор
правых частей размером N, x–искомый вектор решений размером N. Решение
системы уравнений итерационным методом состоит в выполнении
следующих шагов.
2. Задается x0 – произвольное начальное приближение решения (вектор с
произвольными начальными значениями).
3. Приближение многократно улучшается с использованием формулы
вида xn+1 = f(xn), где функция f определяется используемым методом1.
4. Процесс продолжается, пока не выполнится условие g(xn) < ε, где
функция g определяется используемым методом, а величина ε задает
требуемую точность.

5. Написать программу на языке C или C++, которая реализует
итерационный алгоритм решения системы линейных алгебраических
уравнений вида Ax=b в соответствии с выбранным вариантом. Здесь A
– матрица размером N×N, x и b – векторы длины N. Тип элементов –
double.
6. Программу распараллелить с помощью MPI с разрезанием матрицы A
по строкам на близкие по размеру, возможно не одинаковые, части.
Соседние строки матрицы должны располагаться в одном или в
соседних MPI-процессах. Реализовать два варианта программы:
- Вариант 1: векторы x и b дублируются в каждом MPI-процессе,
- Вариант 2: векторы x и b разрезаются между MPI-процессами
аналогично матрице A
7. Замерить время работы двух вариантов программы при использовании
различного числа процессорных ядер: 1,2, 4, 8, 16. Построить графики
зависимости времени работы программы, ускорения и эффективности
распараллеливания от числа используемых ядер. Исходные данные,
параметры N и ε подобрать таким образом, чтобы решение задачи на
одном ядре занимало не менее 30 секунд.
8. Выполнить профилирование двух вариантов программы с помощью
MPE при использовании 16-и ядер.
9. На основании полученных результатов сделать вывод о
целесообразности использования одного или второго варианта
программы.
## метод простой итерации:
Ax = b
A - матрица коэффициентов уравнений размеров NxN
b - вектор правых частей
x - искомый вектор
eps - точность
tau - параметр метода

## Для примера:
N = 5000
TAU = 0.01
&
N = 100
TAU = 0.01
## Компиляция

main.c - последовательная программа:
    gcc main.c -o main.out -lm
    ./main.out
var1.c - первый вариант оптимизации, векторы x и b дублируются в каждом MPI-процессе:
    mpicc var1.c -o var1.out -lm
    mpirun -np N ./var1.out
    N - кол-во ядер

var2.c - второй вариант оптимизации, где векторы x и b разрезаются между MPI-процессами аналогично матрице A:
    mpicc var2.c -o var2.out -lm
    mpirun -np N ./var2.out
    N - кол-во ядер


## Время выполнения:
(кафедральный сервер)
#define N 5000
#define EPSILON 0.000001
#define TAU 0.00001
#define MAX_ITERATION_COUNT 100000

Main: 36.969714 sec

Var1:
    1 thread: 36.389871 sec
    2 threads: 18.742376 sec
    4 threads: 9.244554 sec
    8 threads: 4.911699 sec
    16 threads: 4.619284 sec

Var2:
    1 thread: 48.718565 sec
    2 threads: 25.879797 sec
    4 threads: 12.426162 sec
    8 threads: 6.846443 sec
    16 threads: 6.262918 sec


# Выводы:
первый вариант параллельной программы целесообразнее в использовании, что доказывает более быстрое исполнение программы, а также показатели ускорения и эффективности, особенно на большом количестве процессов

# Теория

Коммуникации между процессорами - передача данных и/или синхронизация
Схема шагов MPI-операции:
– шаг 1 «инициализация» – список аргументов принят к уче-
ту в реализации операции, но содержимое буферов данных
на этом этапе не рассматривается;
– шаг 2 «старт» – в рассмотрение вовлекаются буферы данных;
– шаг 3 «завершение» – возвращает управление буферами и ар-
гументами в программу – данные в буферах и аргументах (по
ссылкам/указателям) обновлены;
– шаг 4 «освобождение» – возвращает управление над всеми ар-
гументами.

Процедура MPI_Init выполняет инициализацию библиотеки MPI
и должна быть вызвана до вызова какой-либо другой MPI-проце-
дуры. Соответственно, процедура MPI_Finalize завершает ра-
боту библиотеки MPI и должна быть вызвана в конце программы.

Константа MPI_COMM_WORLD обозначает группу всех запущенных
процессов (в терминах MPI – «коммуникатор»). Процедура MPI_
Comm_size возвращает количество процессов в коммуникаторе,
а процедура MPI_Comm_rank – номер данного процесса в комму-
никаторе.

Процессы в MPI могут образовывать группы. Кроме того, на груп-
пе процессов может быть задан граф связей с определенной топо-
логией. Этот граф задает пользователь, обозначая таким образом
пути коммуникаций в параллельной программе (не обязательно
соответствующие реальным взаимодействиям). Библиотека MPI,
в свою очередь, старается наилучшим образом отобразить этот
граф процессов (виртуальную топологию) на физическую архитек-
туру параллельной вычислительной системы. В библиотеке MPI
группа процессов с заданной на ней виртуальной топологией связи
называется ***коммуникатором***. Каждый процесс может принадле-
жать нескольким коммуникаторам и иметь в каждом из них свой
номер. В параметрах MPI-процедур вместе с номерами процессов
всегда указывается коммуникатор, в рамках которого действуют
эти номера. Дескриптор коммуникатора в программе имеет тип
MPI_Comm.

При запуске программы изначально создается общий комму-
никатор (MPI_COMM_WORLD), включающий все процессы, с то-
пологией связи «все со всеми». На базе существующих коммуни-
каторов пользователь может конструировать новые (например,
с помощью процедур MPI_Cart_create, MPI_Graph_create).
Коммуникаторы типа «граф» позволяют строить группы процес-
сов с произвольной топологией связи. Коммуникаторы с декарто-
вой топологией (линейки, решетки, торы, …) удобно использовать
для создания параллельных программ с регулярной структурой
процессов

** `MPI_Init(&argc, &argv)`**
Инициализация MPI. Вызывать в начале программы.

**`MPI_Comm_size(MPI_COMM_WORLD, &process_count)`**
Узнать, сколько всего процессов.

**`MPI_Comm_rank(MPI_COMM_WORLD, &process_rank)`**
Узнать номер текущего процесса (от 0 до N-1).

**`MPI_Wtime()`**
Вернуть текущее время (для замера времени работы).

**`MPI_Allgatherv(...)`**
Каждый процесс отправляет данные → все получают всё.

**`MPI_Reduce(send, recv, count, type, op, root, comm)`**

Собирает данные со всех процессов, применяет операцию (например, сумму) и отправляет на `root`.

**`MPI_Bcast(buf, count, type, root, comm)`**
`root` рассылает данные всем остальным процессам.

** `MPI_Sendrecv_replace(buf, count, type, dest, stag, src, rtag, comm, status)`**
Одновременно отправляет и получает данные в **один и тот же буфер**.

---